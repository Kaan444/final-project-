{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fdd0bcb",
   "metadata": {},
   "source": [
    "# [STARTER] Udaplay Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9325b035",
   "metadata": {},
   "source": [
    "## Part 02 - Agent\n",
    "\n",
    "In this part of the project, you'll use your VectorDB to be part of your Agent as a tool.\n",
    "\n",
    "You're building UdaPlay, an AI Research Agent for the video game industry. The agent will:\n",
    "1. Answer questions using internal knowledge (RAG)\n",
    "2. Search the web when needed\n",
    "3. Maintain conversation state\n",
    "4. Return structured outputs\n",
    "5. Store useful information for future use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b42de90",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd10c06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import the necessary libs\n",
    "# For example: \n",
    "import os\n",
    "\n",
    "from lib.agents import Agent\n",
    "from lib.llm import LLM\n",
    "from lib.messages import UserMessage, SystemMessage, ToolMessage, AIMessage\n",
    "from lib.tooling import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e465d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('config.env')\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27de4729",
   "metadata": {},
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ab2dac",
   "metadata": {},
   "source": [
    "Build at least 3 tools:\n",
    "- retrieve_game: To search the vector DB\n",
    "- evaluate_retrieval: To assess the retrieval performance\n",
    "- game_web_search: If no good, search the web\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4f14cd",
   "metadata": {},
   "source": [
    "#### Retrieve Game Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25c36dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from lib.tooling import tool\n",
    "\n",
    "@tool\n",
    "def retrieve_game(query: str, k: int = 5):\n",
    "    \"\"\"\n",
    "    Semantic search: Finds most results in the vector DB\n",
    "\n",
    "    args:\n",
    "    - query: a question about game industry.\n",
    "\n",
    "    You'll receive results as list. Each element contains:\n",
    "    - Platform: like Game Boy, Playstation 5, Xbox 360...)\n",
    "    - Name: Name of the Game\n",
    "    - YearOfRelease: Year when that game was released for that platform\n",
    "    - Description: Additional details about the game\n",
    "    \"\"\"\n",
    "    # ---- Chroma client + embedding fn (Vacareum/OpenAI-compatible) ----\n",
    "    chroma_client = chromadb.PersistentClient(path=\"chromadb\")\n",
    "\n",
    "    api_key = (os.getenv(\"OPENAI_API_KEY\") or \"\").strip()\n",
    "    api_base = (os.getenv(\"OPENAI_BASE_URL\") or \"\").strip()\n",
    "\n",
    "    if not api_key:\n",
    "        raise ValueError(\"Missing OPENAI_API_KEY in environment.\")\n",
    "    if not api_base:\n",
    "        raise ValueError(\"Missing OPENAI_BASE_URL in environment (Vacareum requires this).\")\n",
    "\n",
    "    embedding_fn = embedding_functions.OpenAIEmbeddingFunction(\n",
    "        api_key=api_key,\n",
    "        api_base=api_base,\n",
    "        model_name=\"text-embedding-3-small\",\n",
    "    )\n",
    "\n",
    "    # Use get_collection because it should already exist from Part 01\n",
    "    collection = chroma_client.get_collection(\n",
    "        name=\"udaplay\",\n",
    "        embedding_function=embedding_fn,\n",
    "    )\n",
    "\n",
    "    # ---- Query ----\n",
    "    res = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=int(k),\n",
    "        include=[\"metadatas\", \"documents\", \"distances\"],\n",
    "    )\n",
    "\n",
    "    metadatas = (res.get(\"metadatas\") or [[]])[0]\n",
    "    documents = (res.get(\"documents\") or [[]])[0]\n",
    "    distances = (res.get(\"distances\") or [[]])[0]\n",
    "\n",
    "    # ---- Format results into the requested schema ----\n",
    "    results = []\n",
    "    for md, doc, dist in zip(metadatas, documents, distances):\n",
    "        md = md or {}\n",
    "        results.append(\n",
    "            {\n",
    "                \"Platform\": md.get(\"Platform\"),\n",
    "                \"Name\": md.get(\"Name\"),\n",
    "                \"YearOfRelease\": md.get(\"YearOfRelease\"),\n",
    "                \"Description\": md.get(\"Description\") or doc,\n",
    "                # helpful extra signal for evaluation/routing\n",
    "                \"distance\": dist,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910dc945",
   "metadata": {},
   "source": [
    "#### Evaluate Retrieval Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9d014b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from lib.tooling import tool\n",
    "import os\n",
    "import json\n",
    "\n",
    "# If Udacity already provides EvaluationReport somewhere, you can replace this\n",
    "# with: from lib.<wherever> import EvaluationReport\n",
    "class EvaluationReport(BaseModel):\n",
    "    useful: bool = Field(..., description=\"Whether the retrieved docs are sufficient to answer the question.\")\n",
    "    description: str = Field(..., description=\"Detailed reasoning explaining what is missing or why it's sufficient.\")\n",
    "\n",
    "@tool\n",
    "def evaluate_retrieval(question: str, retrieved_docs: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Based on the user's question and on the list of retrieved documents,\n",
    "    it will analyze the usability of the documents to respond to that question.\n",
    "\n",
    "    args:\n",
    "    - question: original question from user\n",
    "    - retrieved_docs: retrieved documents most similar to the user query in the Vector Database\n",
    "\n",
    "    The result includes:\n",
    "    - useful: whether the documents are useful to answer the question\n",
    "    - description: description about the evaluation result\n",
    "    \"\"\"\n",
    "    # ---- OpenAI/Vacareum compatible client ----\n",
    "    api_key = (os.getenv(\"OPENAI_API_KEY\") or \"\").strip()\n",
    "    base_url = (os.getenv(\"OPENAI_BASE_URL\") or \"\").strip()  # Vacareum needs this\n",
    "    if not api_key:\n",
    "        raise ValueError(\"Missing OPENAI_API_KEY in environment.\")\n",
    "    if not base_url:\n",
    "        raise ValueError(\"Missing OPENAI_BASE_URL in environment (Vacareum requires this).\")\n",
    "\n",
    "    try:\n",
    "        from openai import OpenAI\n",
    "    except ImportError as e:\n",
    "        raise ImportError(\"openai package not installed. Run: pip install openai\") from e\n",
    "\n",
    "    client = OpenAI(api_key=api_key, base_url=base_url)\n",
    "\n",
    "    # Keep prompt small + stable\n",
    "    # Only include the most relevant fields\n",
    "    compact_docs = []\n",
    "    for d in (retrieved_docs or [])[:8]:\n",
    "        compact_docs.append(\n",
    "            {\n",
    "                \"Platform\": d.get(\"Platform\"),\n",
    "                \"Name\": d.get(\"Name\"),\n",
    "                \"YearOfRelease\": d.get(\"YearOfRelease\"),\n",
    "                \"Description\": (d.get(\"Description\") or \"\")[:300],\n",
    "                \"distance\": d.get(\"distance\"),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    system = (\n",
    "        \"You are a strict retrieval evaluator for a game-industry RAG system. \"\n",
    "        \"Decide if the retrieved documents are sufficient to answer the user's question accurately. \"\n",
    "        \"If the question asks about something not clearly supported (missing game, missing platform, \"\n",
    "        \"missing release year, etc.), mark useful=false and explain what’s missing.\"\n",
    "    )\n",
    "\n",
    "    user = (\n",
    "        \"Return ONLY valid JSON with exactly these keys:\\n\"\n",
    "        '  {\"useful\": boolean, \"description\": string}\\n\\n'\n",
    "        f\"Question:\\n{question}\\n\\n\"\n",
    "        f\"Retrieved documents (may be empty):\\n{json.dumps(compact_docs, ensure_ascii=False)}\\n\"\n",
    "    )\n",
    "\n",
    "    # Use a widely supported chat call (works with OpenAI-compatible proxies)\n",
    "    resp = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",  # if your course specifies a model, replace here\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": user},\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )\n",
    "\n",
    "    text = (resp.choices[0].message.content or \"\").strip()\n",
    "\n",
    "    # Parse JSON robustly\n",
    "    try:\n",
    "        data = json.loads(text)\n",
    "        report = EvaluationReport(**data)\n",
    "        return report.model_dump()\n",
    "    except (json.JSONDecodeError, ValidationError):\n",
    "        # Fallback: attempt to extract JSON block if model wrapped it\n",
    "        start = text.find(\"{\")\n",
    "        end = text.rfind(\"}\")\n",
    "        if start != -1 and end != -1 and end > start:\n",
    "            try:\n",
    "                data = json.loads(text[start : end + 1])\n",
    "                report = EvaluationReport(**data)\n",
    "                return report.model_dump()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        # If parsing fails, return a conservative result\n",
    "        return EvaluationReport(\n",
    "            useful=False,\n",
    "            description=(\n",
    "                \"Evaluator could not produce valid JSON. Raw output was:\\n\"\n",
    "                + text[:800]\n",
    "            ),\n",
    "        ).model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7935a26",
   "metadata": {},
   "source": [
    "#### Game Web Search Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad698aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Any\n",
    "from lib.tooling import tool\n",
    "\n",
    "@tool\n",
    "def game_web_search(question: str, max_results: int = 5) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Web search: Uses Tavily to search the web for game-industry questions.\n",
    "\n",
    "    args:\n",
    "    - question: a question about game industry.\n",
    "\n",
    "    returns: a list of results, each containing:\n",
    "    - title: page title\n",
    "    - url: page url\n",
    "    - content: short snippet / extracted content (if available)\n",
    "    - score: relevance score (if provided)\n",
    "    \"\"\"\n",
    "    api_key = (os.getenv(\"TAVILY_API_KEY\") or \"\").strip()\n",
    "    if not api_key:\n",
    "        raise ValueError(\"Missing TAVILY_API_KEY in environment (.env).\")\n",
    "\n",
    "    try:\n",
    "        # tavily-python client\n",
    "        from tavily import TavilyClient\n",
    "    except ImportError as e:\n",
    "        raise ImportError(\"Missing Tavily client. Install with: pip install tavily-python\") from e\n",
    "\n",
    "    client = TavilyClient(api_key=api_key)\n",
    "\n",
    "    # Tavily returns a dict with a \"results\" list\n",
    "    resp = client.search(\n",
    "        query=question,\n",
    "        max_results=int(max_results),\n",
    "        # these are commonly supported; if your version errors, remove them\n",
    "        include_answer=False,\n",
    "        include_raw_content=False,\n",
    "        search_depth=\"basic\",\n",
    "    )\n",
    "\n",
    "    results = []\n",
    "    for r in (resp.get(\"results\") or []):\n",
    "        results.append(\n",
    "            {\n",
    "                \"title\": r.get(\"title\"),\n",
    "                \"url\": r.get(\"url\"),\n",
    "                \"content\": r.get(\"content\") or r.get(\"snippet\"),\n",
    "                \"score\": r.get(\"score\"),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df844b3b",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c56281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import TypedDict, List, Dict, Any\n",
    "from dotenv import load_dotenv\n",
    "import contextlib\n",
    "import io\n",
    "\n",
    "from lib.llm import LLM\n",
    "from lib.state_machine import StateMachine, EntryPoint, Termination, Transition, Step\n",
    "from lib.messages import SystemMessage, UserMessage\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "OPENAI_API_KEY = (os.getenv(\"OPENAI_API_KEY\") or \"\").strip()\n",
    "OPENAI_BASE_URL = (os.getenv(\"OPENAI_BASE_URL\") or \"\").strip()\n",
    "TAVILY_API_KEY = (os.getenv(\"TAVILY_API_KEY\") or \"\").strip()\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"Missing OPENAI_API_KEY\")\n",
    "if not OPENAI_BASE_URL:\n",
    "    raise ValueError(\"Missing OPENAI_BASE_URL (Vacareum requires this)\")\n",
    "if not TAVILY_API_KEY:\n",
    "    raise ValueError(\"Missing TAVILY_API_KEY\")\n",
    "\n",
    "llm = LLM(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY)\n",
    "\n",
    "INSTRUCTIONS = \"\"\"\n",
    "You are UdaPlay, an AI research agent for the video game industry.\n",
    "\n",
    "Goals:\n",
    "1) Prefer internal knowledge (Vector DB) via retrieve_game (RAG).\n",
    "2) Evaluate whether retrieved docs are sufficient.\n",
    "3) If insufficient, search the web using game_web_search.\n",
    "4) Be precise about platforms and release years.\n",
    "5) If information is missing, say so clearly.\n",
    "\n",
    "Answer style:\n",
    "- Start with a concise direct answer.\n",
    "- Follow with 2–4 bullet points citing either:\n",
    "  • retrieved game metadata, or\n",
    "  • web sources (title + URL).\n",
    "\"\"\"\n",
    "\n",
    "class UdaPlayState(TypedDict, total=False):\n",
    "    question: str\n",
    "    retrieved_docs: List[Dict[str, Any]]\n",
    "    retrieval_report: Dict[str, Any]\n",
    "    web_results: List[Dict[str, Any]]\n",
    "    final_answer: str\n",
    "\n",
    "\n",
    "def udaplay_workflow(state: UdaPlayState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    IMPORTANT: Step.run expects this function to return a DICT OF UPDATES,\n",
    "    not necessarily the full state.\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    retrieved = retrieve_game(question)\n",
    "    report = evaluate_retrieval(question, retrieved)\n",
    "\n",
    "    updates: Dict[str, Any] = {\n",
    "        \"retrieved_docs\": retrieved,\n",
    "        \"retrieval_report\": report,\n",
    "    }\n",
    "\n",
    "    if not report.get(\"useful\", False):\n",
    "        web_results = game_web_search(question)\n",
    "        updates[\"web_results\"] = web_results\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "User question:\n",
    "{question}\n",
    "\n",
    "Web search results:\n",
    "{web_results}\n",
    "\n",
    "Evaluator report:\n",
    "{report}\n",
    "\n",
    "Answer using the web results.\n",
    "Include sources (title + URL).\n",
    "\"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"\n",
    "User question:\n",
    "{question}\n",
    "\n",
    "Retrieved documents:\n",
    "{retrieved}\n",
    "\n",
    "Evaluator report:\n",
    "{report}\n",
    "\n",
    "Answer using ONLY the retrieved documents.\n",
    "If information is missing, explain what is missing.\n",
    "\"\"\"\n",
    "\n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content=INSTRUCTIONS),\n",
    "        UserMessage(content=prompt),\n",
    "    ])\n",
    "\n",
    "    updates[\"final_answer\"] = response\n",
    "    return updates\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Build StateMachine correctly\n",
    "# -----------------------------\n",
    "sm = StateMachine(UdaPlayState)\n",
    "\n",
    "start = EntryPoint()\n",
    "start.step_id = \"START\"  # marker step (logic is empty by design)\n",
    "\n",
    "work = Step(\"WORK\", udaplay_workflow)  # real business logic step\n",
    "\n",
    "end = Termination()\n",
    "end.step_id = \"END\"  # termination marker\n",
    "\n",
    "sm.add_steps([start, work, end])\n",
    "\n",
    "# connect uses: connect(source, targets, condition=None)  :contentReference[oaicite:2]{index=2}\n",
    "sm.connect(\"START\", \"WORK\")\n",
    "sm.connect(\"WORK\", \"END\")\n",
    "sm.connect(\"END\", [])  # optional; Termination breaks before transition lookup\n",
    "\n",
    "# -----------------------------\n",
    "# Run + print final answer\n",
    "# -----------------------------\n",
    "with contextlib.redirect_stdout(io.StringIO()):\n",
    "    run = sm.run({\"question\": \"When Pokémon Gold and Silver was released?\"})\n",
    "\n",
    "final_state = run.get_final_state() or {}\n",
    "ans = final_state.get(\"final_answer\")\n",
    "print(ans.content if hasattr(ans, \"content\") else ans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec23893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib, io\n",
    "\n",
    "# ---------------------------------------\n",
    "# Invoke your agent (question + answer only)\n",
    "# ---------------------------------------\n",
    "\n",
    "questions = [\n",
    "    \"When Pokémon Gold and Silver was released?\",\n",
    "    \"Which one was the first 3D platformer Mario game?\",\n",
    "    \"Was Mortal Kombat X released for PlayStation 5?\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    # Silence Udacity StateMachine internal prints\n",
    "    with contextlib.redirect_stdout(io.StringIO()):\n",
    "        run = sm.run({\"question\": q})\n",
    "\n",
    "    final_state = run.get_final_state() or {}\n",
    "    ans = final_state.get(\"final_answer\")\n",
    "\n",
    "    print(\"QUESTION:\", q)\n",
    "    print(\"ANSWER:\", ans.content if hasattr(ans, \"content\") else ans)\n",
    "    print()  # blank line between Q&As\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a55081",
   "metadata": {},
   "source": [
    "### (Optional) Advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb83fbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Update your agent with long-term memory\n",
    "# TODO: Convert the agent to be a state machine, with the tools being pre-defined nodes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
